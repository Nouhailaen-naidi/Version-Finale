# -*- coding: utf-8 -*-
"""Analyse du Dataset : Global EV Sales (2010-2024).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F87D8CiOSIqQC0MOjXWkNb6DWX2D7PVj

1) IMPORTATION + D√âZIPPAGE + LECTURE AUTOMATIQUE DU CSV
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Nom du fichier upload√©
zip_filename = "archive (3) (1).zip"

# D√©zipper dans un dossier "data"
with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall("data")

print("Fichiers extraits :")
os.listdir("data")

import pandas as pd

# Trouver le fichier CSV automatiquement
csv_files = [f for f in os.listdir("data") if f.endswith(".csv")]

if len(csv_files) == 0:
    print("Aucun fichier CSV trouv√©.")
else:
    csv_file = "data/" + csv_files[0]
    print("CSV d√©tect√© :", csv_file)

    df = pd.read_csv(csv_file)
    print("Shape:", df.shape)
    df.head()

# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter

# ================================
# 1) UPLOAD + EXTRACTION DU ZIP
# ================================
from google.colab import files
import zipfile
import os
import pandas as pd

uploaded = files.upload()

# D√©tection automatique du zip
zip_filename = list(uploaded.keys())[0]

# Extraction
with zipfile.ZipFile(zip_filename, 'r') as z:
    z.extractall("data")

print("Fichiers extraits :", os.listdir("data"))

# ================================
# 2) LECTURE AUTOMATIQUE DU CSV
# ================================
csv_files = [f for f in os.listdir("data") if f.endswith(".csv")]

if len(csv_files) == 0:
    raise Exception("Aucun CSV d√©tect√© dans votre fichier ZIP !")

csv_file = "data/" + csv_files[0]
print("CSV trouv√© :", csv_file)

df = pd.read_csv(csv_file)
df.head()

# ================================
# PREPROCESSING COMPLET
# ================================
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

print("Shape initial :", df.shape)

# 1. Supprimer les doublons
df = df.drop_duplicates()
print("Apr√®s suppression des doublons :", df.shape)

# 2. S√©paration des colonnes
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

print("Colonnes num√©riques :", num_cols)
print("Colonnes cat√©gorielles :", cat_cols)

# 3. Gestion des valeurs manquantes
df[num_cols] = df[num_cols].fillna(df[num_cols].median())
df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])

# üéØ Identifier la target
target = "y" if "y" in df.columns else df.columns[-1]

X = df.drop(columns=[target])
y = df[target].apply(lambda x: 1 if str(x).lower()=="yes" else 0)

# 4. Encodage + Normalisation via Pipeline
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols),
    ]
)

print("=== Pr√©processing termin√© ===")

# ================================
# EDA COMPLET
# ================================
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

# ----------- A. Distribution de la target -----------
plt.figure(figsize=(6,4))
y.value_counts().plot(kind="bar")
plt.title("Distribution de la variable cible")
plt.xlabel("Classe")
plt.ylabel("Fr√©quence")
plt.show()

print("""
INTERPR√âTATION :
Si les classes sont tr√®s d√©s√©quilibr√©es, cela signifie que le mod√®le pourrait
avoir du mal √† pr√©dire la classe minoritaire. Un r√©√©quilibrage sera n√©cessaire.
""")

# ----------- B. Histogrammes des variables num√©riques -----------
df[num_cols].hist(bins=30, figsize=(15,8))
plt.suptitle("Distribution des variables num√©riques")
plt.show()

print("""
INTERPR√âTATION :
Ces distributions permettent d'identifier les asym√©tries, outliers et transformations
potentielles (log-transform, normalisation‚Ä¶).
""")

# ----------- C. Boxplots pour d√©tecter les outliers -----------
plt.figure(figsize=(15,8))
df[num_cols].plot(kind="box", subplots=True, layout=(4,4), figsize=(15,10))
plt.suptitle("D√©tection d'Outliers")
plt.show()

print("""
INTERPR√âTATION :
Les boxplots montrent les valeurs extr√™mes (outliers) qui peuvent influencer
n√©gativement la performance des mod√®les sensibles aux √©chelles.
""")

# ----------- D. Heatmap des corr√©lations -----------
plt.figure(figsize=(10,8))
corr = df[num_cols].corr()
sns.heatmap(corr, cmap="coolwarm", annot=False)
plt.title("Corr√©lations entre variables num√©riques")
plt.show()

print("""
INTERPR√âTATION :
La heatmap montre les relations lin√©aires entre variables.
Une forte corr√©lation indique des variables redondantes qu‚Äôon peut √©liminer
ou combiner.
""")

# ----------- E. Barplots pour les colonnes cat√©gorielles -----------
for col in cat_cols:
    plt.figure(figsize=(10,4))
    df[col].value_counts().plot(kind="bar")
    plt.title(f"Distribution de {col}")
    plt.xticks(rotation=45)
    plt.show()

    print(f"""
INTERPR√âTATION ({col}) :
Ce graphique montre les cat√©gories les plus fr√©quentes. Cela permet de comprendre
la composition d√©mographique / comportementale du dataset.
""")

# ================================
# TRAIN / TEST SPLIT
# ================================
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print("Taille Train :", X_train.shape)
print("Taille Test  :", X_test.shape)