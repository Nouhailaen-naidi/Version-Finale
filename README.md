# ğŸ“Š Analyse du Dataset : Global EV Sales (2010-2024)


![NOUHAILA](https://github.com/user-attachments/assets/1a6f9a52-0683-446b-923b-b4a4b38d8c51)

EN-NAIDI NOUHAILA 22006162

## Rapport Complet d'Analyse Exploratoire des DonnÃ©es

---

## ğŸ“‘ Table des MatiÃ¨res

1. [Introduction](#-1-introduction)
2. [Le Dataset](#-2-le-dataset)
3. [Chargement et Exploration](#-3-chargement-et-exploration-initiale)
4. [Nettoyage des DonnÃ©es](#-4-nettoyage-des-donnÃ©es)
5. [Analyse Exploratoire (EDA)](#-5-analyse-exploratoire-des-donnÃ©es)
6. [ModÃ©lisation](#-6-prÃ©paration-pour-la-modÃ©lisation)
7. [Conclusions](#-7-conclusions-et-recommandations)

---

## ğŸ¯ 1. Introduction

### Contexte du Projet

Ce projet analyse l'Ã©volution mondiale des **ventes de vÃ©hicules Ã©lectriques (EV)** de 2010 Ã  2024. L'objectif principal est de comprendre :

- ğŸŒ La croissance du marchÃ© EV mondial
- ğŸ“ˆ Les tendances par rÃ©gion et par pays
- ğŸ”‹ L'Ã©volution des types de motorisation (BEV vs PHEV)
- ğŸ¯ Les facteurs influenÃ§ant l'adoption des vÃ©hicules Ã©lectriques

### Objectifs de l'Analyse

âœ… **Analyse descriptive** : Comprendre la structure et l'Ã©volution du marchÃ©  
âœ… **Analyse comparative** : Identifier les leaders et les tendances rÃ©gionales  
âœ… **PrÃ©paration des donnÃ©es** : Nettoyer et structurer pour la modÃ©lisation  
âœ… **Insights business** : Fournir des recommandations stratÃ©giques  

---

## ğŸ“ 2. Le Dataset

### 2.1 Source et Provenance

| Ã‰lÃ©ment | DÃ©tail |
|---------|--------|
| **Plateforme** | Kaggle |
| **Nom du dataset** | Global EV Sales 2010-2024 |
| **Auteur** | Patrick L. Ford |
| **Lien** | [Kaggle Dataset](https://www.kaggle.com/datasets/patricklford/global-ev-sales-2010-2024) |
| **PÃ©riode couverte** | 2010 â†’ 2024 (15 ans) |
| **ThÃ©matique** | MobilitÃ© Ã©lectrique, transition Ã©nergÃ©tique |
| **Licence** | Open Data |

### 2.2 DÃ©finition de la ProblÃ©matique

Ce dataset permet de rÃ©pondre Ã  **plusieurs types de problÃ¨mes en Machine Learning** :

#### ğŸ”µ **ProblÃ¨me 1 : RÃ©gression**
- **Type** : RÃ©gression supervisÃ©e
- **Variable cible** : `EV_Sales` (nombre de vÃ©hicules Ã©lectriques vendus)
- **Variables explicatives** : Year, Country, Type, Market_Share, etc.
- **Objectif** : PrÃ©dire les ventes futures selon diffÃ©rents facteurs
- **Algorithmes applicables** : 
  - RÃ©gression LinÃ©aire Multiple
  - Random Forest Regressor
  - XGBoost
  - Gradient Boosting

#### ğŸŸ¢ **ProblÃ¨me 2 : SÃ©ries Temporelles (Time Series Forecasting)**
- **Type** : PrÃ©vision temporelle
- **Variable cible** : `EV_Sales` dans le temps
- **Objectif** : Forecasting des ventes pour 2025-2030
- **Algorithmes applicables** :
  - ARIMA / SARIMA
  - Prophet (Facebook)
  - LSTM (Deep Learning)
  - Exponential Smoothing

#### ğŸŸ¡ **ProblÃ¨me 3 : Clustering (Non-supervisÃ©)**
- **Type** : Segmentation
- **Objectif** : Regrouper les pays selon leurs patterns d'adoption EV
- **Variables** : Taux de croissance, Market Share, EV Stock
- **Algorithmes applicables** :
  - K-Means
  - DBSCAN
  - Clustering HiÃ©rarchique

### 2.3 MÃ©tadonnÃ©es du Dataset

#### Structure GÃ©nÃ©rale

| Ã‰lÃ©ment | Valeur EstimÃ©e |
|---------|----------------|
| **Nombre de lignes** | ~200-500 (selon version) |
| **Nombre de colonnes** | 6-8 variables |
| **Taille mÃ©moire** | < 1 MB |
| **PÃ©riode** | 2010 â†’ 2024 |
| **FrÃ©quence** | Annuelle |
| **GranularitÃ©** | Pays/RÃ©gion Ã— AnnÃ©e Ã— Type |
| **UnitÃ© de mesure** | Nombre de vÃ©hicules vendus |

### 2.4 Dictionnaire des Variables

| Variable | RÃ´le | Type | Description | Exemples | Valeurs Manquantes |
|----------|------|------|-------------|----------|-------------------|
| **year** | Feature | NumÃ©rique (int) | AnnÃ©e de rÃ©fÃ©rence | 2010, 2015, 2024 | âŒ Non |
| **country** | Feature | CatÃ©gorielle | Pays ou rÃ©gion gÃ©ographique | China, USA, Europe, India | âŒ Non |
| **region** | Feature | CatÃ©gorielle | Zone gÃ©ographique agrÃ©gÃ©e | Asia, North America, Europe | âš ï¸ Possibles |
| **ev_sales** | **Target** | NumÃ©rique (int) | Nombre de VE vendus dans l'annÃ©e | 50000, 1500000 | âš ï¸ Possibles |
| **type** | Feature | CatÃ©gorielle | Type de motorisation | BEV, PHEV, Total | âŒ Non |
| **market_share** | Feature | NumÃ©rique (float) | Part de marchÃ© (% du total auto) | 0.05 (5%), 0.18 (18%) | âš ï¸ Possibles |
| **ev_stock** | Feature | NumÃ©rique (int) | Parc total de VE en circulation | 500000, 10000000 | âš ï¸ Possibles |

#### ğŸ“– Glossaire des Types de VÃ©hicules

- **BEV** (Battery Electric Vehicle) : VÃ©hicule 100% Ã©lectrique Ã  batterie
- **PHEV** (Plug-in Hybrid Electric Vehicle) : VÃ©hicule hybride rechargeable
- **Total** : Somme des BEV + PHEV

---

## ğŸ’» 3. Chargement et Exploration Initiale

### 3.1 Installation des DÃ©pendances
```python
# ===============================================
# Installation des packages nÃ©cessaires
# ===============================================
!pip install kagglehub[pandas-datasets]
!pip install pandas numpy matplotlib seaborn scikit-learn

print("âœ… Installation terminÃ©e")
```

### 3.2 Importation des BibliothÃ¨ques
```python
# ===============================================
# Importation des bibliothÃ¨ques
# ===============================================
import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Configuration
warnings.filterwarnings('ignore')
sns.set_theme(style="whitegrid", palette="husl")
plt.rcParams['figure.figsize'] = (14, 7)
plt.rcParams['font.size'] = 11

print("âœ… BibliothÃ¨ques importÃ©es avec succÃ¨s")
```

### 3.3 Chargement du Dataset
```python
# ===============================================
# Chargement via KaggleHub
# ===============================================
file_path = ""  # Laissez vide pour charger le fichier principal automatiquement

df = kagglehub.load_dataset(
    KaggleDatasetAdapter.PANDAS,
    "patricklford/global-ev-sales-2010-2024",
    file_path,
)

print("="*80)
print(" " * 25 + "DATASET CHARGÃ‰ AVEC SUCCÃˆS")
print("="*80)
print(f"\nğŸ“Š Dimensions : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
print(f"\nğŸ” AperÃ§u des premiÃ¨res lignes :")
df.head(10)
```

### 3.4 Exploration Initiale
```python
# ===============================================
# Informations GÃ©nÃ©rales du Dataset
# ===============================================
print("\n" + "="*80)
print(" " * 30 + "INFORMATIONS GÃ‰NÃ‰RALES")
print("="*80)

# Structure
print(f"\nğŸ“ STRUCTURE DU DATASET")
print(f"{'â”€'*80}")
print(f"   â€¢ Nombre de lignes : {df.shape[0]:,}")
print(f"   â€¢ Nombre de colonnes : {df.shape[1]}")
print(f"   â€¢ Taille mÃ©moire : {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# PÃ©riode temporelle
print(f"\nğŸ“… PÃ‰RIODE TEMPORELLE")
print(f"{'â”€'*80}")
if 'Year' in df.columns or 'year' in df.columns:
    year_col = 'Year' if 'Year' in df.columns else 'year'
    print(f"   â€¢ AnnÃ©e minimum : {df[year_col].min()}")
    print(f"   â€¢ AnnÃ©e maximum : {df[year_col].max()}")
    print(f"   â€¢ Ã‰tendue : {df[year_col].max() - df[year_col].min() + 1} ans")

# Couverture gÃ©ographique
print(f"\nğŸŒ COUVERTURE GÃ‰OGRAPHIQUE")
print(f"{'â”€'*80}")
country_cols = [col for col in df.columns if 'country' in col.lower()]
if country_cols:
    country_col = country_cols[0]
    print(f"   â€¢ Nombre de pays/rÃ©gions : {df[country_col].nunique()}")
    print(f"   â€¢ Liste des pays (top 10) : {', '.join(df[country_col].unique()[:10])}")

# Types de donnÃ©es
print(f"\nğŸ“‹ TYPES DE DONNÃ‰ES")
print(f"{'â”€'*80}")
print(df.dtypes)

# Valeurs manquantes
print(f"\nâ“ VALEURS MANQUANTES")
print(f"{'â”€'*80}")
missing = df.isnull().sum()
if missing.sum() == 0:
    print("   âœ… Aucune valeur manquante dÃ©tectÃ©e")
else:
    missing_df = pd.DataFrame({
        'Colonne': missing.index,
        'Manquantes': missing.values,
        'Pourcentage': (missing.values / len(df) * 100).round(2)
    })
    print(missing_df[missing_df['Manquantes'] > 0].to_string(index=False))

# Statistiques descriptives
print(f"\nğŸ“Š STATISTIQUES DESCRIPTIVES")
print(f"{'â”€'*80}")
df.describe()
```

**ğŸ“ InterprÃ©tation Initiale** :
- Le dataset couvre 15 annÃ©es d'Ã©volution du marchÃ© EV
- PrÃ©sence de donnÃ©es numÃ©riques et catÃ©gorielles
- VÃ©rification de la qualitÃ© des donnÃ©es nÃ©cessaire avant analyse

---

## ğŸ§¹ 4. Nettoyage des DonnÃ©es

### 4.1 Analyse de la QualitÃ© des DonnÃ©es
```python
# ===============================================
# Diagnostic QualitÃ© des DonnÃ©es
# ===============================================
print("\n" + "="*80)
print(" " * 25 + "ANALYSE DE QUALITÃ‰ DES DONNÃ‰ES")
print("="*80)

# 1. Valeurs manquantes dÃ©taillÃ©es
print("\n1ï¸âƒ£  VALEURS MANQUANTES")
print("â”€"*80)
missing_data = df.isnull().sum()
missing_pct = (missing_data / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing_data.index,
    'Valeurs_Manquantes': missing_data.values,
    'Pourcentage': missing_pct.values
}).sort_values('Valeurs_Manquantes', ascending=False)

if missing_df['Valeurs_Manquantes'].sum() > 0:
    print(missing_df[missing_df['Valeurs_Manquantes'] > 0].to_string(index=False))
else:
    print("   âœ… Aucune valeur manquante")

# 2. Doublons
print("\n2ï¸âƒ£  DOUBLONS")
print("â”€"*80)
duplicates = df.duplicated().sum()
print(f"   â€¢ Nombre de lignes dupliquÃ©es : {duplicates}")
if duplicates > 0:
    print(f"   âš ï¸  {duplicates} doublons dÃ©tectÃ©s - suppression recommandÃ©e")
    print(f"   â€¢ Pourcentage : {(duplicates/len(df)*100):.2f}%")

# 3. Valeurs aberrantes (Outliers)
print("\n3ï¸âƒ£  DÃ‰TECTION DES VALEURS ABERRANTES")
print("â”€"*80)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col].count()
    
    if outliers > 0:
        print(f"   â€¢ {col} : {outliers} valeurs aberrantes dÃ©tectÃ©es")

# 4. CohÃ©rence des donnÃ©es
print("\n4ï¸âƒ£  COHÃ‰RENCE DES DONNÃ‰ES")
print("â”€"*80)

# VÃ©rification des annÃ©es
if 'Year' in df.columns or 'year' in df.columns:
    year_col = 'Year' if 'Year' in df.columns else 'year'
    invalid_years = df[(df[year_col] < 2010) | (df[year_col] > 2024)][year_col].count()
    print(f"   â€¢ AnnÃ©es hors pÃ©riode 2010-2024 : {invalid_years}")

# VÃ©rification des valeurs nÃ©gatives
for col in numeric_cols:
    negative_count = (df[col] < 0).sum()
    if negative_count > 0:
        print(f"   âš ï¸  {col} : {negative_count} valeurs nÃ©gatives (possiblement incorrectes)")

print("\n" + "="*80 + "\n")
```

**ğŸ“Š Visualisation des Valeurs Manquantes**
```python
# ===============================================
# Graphique des valeurs manquantes
# ===============================================
if missing_df['Valeurs_Manquantes'].sum() > 0:
    plt.figure(figsize=(12, 6))
    
    missing_plot = missing_df[missing_df['Valeurs_Manquantes'] > 0]
    
    plt.bar(missing_plot['Colonne'], missing_plot['Pourcentage'], 
            color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5)
    
    plt.title('Pourcentage de Valeurs Manquantes par Colonne', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Colonnes', fontsize=13, fontweight='bold')
    plt.ylabel('Pourcentage de Valeurs Manquantes (%)', fontsize=13, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', alpha=0.3, linestyle='--')
    
    # Ligne de rÃ©fÃ©rence Ã  5%
    plt.axhline(y=5, color='orange', linestyle='--', linewidth=2, 
                label='Seuil critique (5%)')
    plt.legend(fontsize=11)
    
    plt.tight_layout()
    plt.show()
else:
    print("âœ… Aucune visualisation nÃ©cessaire - pas de valeurs manquantes")
```

### 4.2 Processus de Nettoyage
```python
# ===============================================
# Nettoyage Complet du Dataset
# ===============================================
print("\n" + "="*80)
print(" " * 28 + "NETTOYAGE DES DONNÃ‰ES")
print("="*80)

# Copie de travail pour prÃ©server l'original
df_clean = df.copy()
initial_rows = len(df_clean)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 1 : Suppression des doublons
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 1 : Suppression des doublons")
duplicates_before = df_clean.duplicated().sum()
df_clean = df_clean.drop_duplicates()
duplicates_after = df_clean.duplicated().sum()
print(f"   âœ“ Doublons supprimÃ©s : {duplicates_before} â†’ {duplicates_after}")
print(f"   âœ“ Lignes conservÃ©es : {len(df_clean)}/{initial_rows}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 2 : Normalisation des noms de colonnes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 2 : Normalisation des noms de colonnes")
df_clean.columns = df_clean.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')
print(f"   âœ“ Colonnes renommÃ©es : {', '.join(df_clean.columns[:5])}...")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 3 : Conversion des types de donnÃ©es
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 3 : Conversion des types de donnÃ©es")

# AnnÃ©e â†’ entier
if 'year' in df_clean.columns:
    df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce').astype('Int64')
    print(f"   âœ“ 'year' â†’ int64")

# Colonnes numÃ©riques
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()
for col in numeric_cols:
    if col != 'year':
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
print(f"   âœ“ {len(numeric_cols)} colonnes numÃ©riques converties")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 4 : Normalisation des textes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 4 : Normalisation des colonnes catÃ©gorielles")
categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()

for col in categorical_cols:
    df_clean[col] = df_clean[col].str.strip().str.title()
print(f"   âœ“ {len(categorical_cols)} colonnes catÃ©gorielles normalisÃ©es")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 5 : Traitement des valeurs manquantes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 5 : Traitement des valeurs manquantes")
print(f"{'â”€'*80}")

for col in df_clean.columns:
    missing_count = df_clean[col].isnull().sum()
    
    if missing_count > 0:
        missing_pct = (missing_count / len(df_clean)) * 100
        
        # StratÃ©gie selon le pourcentage de valeurs manquantes
        if missing_pct < 5:  # Imputation si < 5%
            if df_clean[col].dtype in ['int64', 'float64', 'Int64']:
                # NumÃ©rique â†’ mÃ©diane
                median_val = df_clean[col].median()
                df_clean[col].fillna(median_val, inplace=True)
                print(f"   âœ“ {col} : {missing_count} valeurs â†’ remplies par mÃ©diane ({median_val:.2f})")
            else:
                # CatÃ©gorielle â†’ mode
                mode_val = df_clean[col].mode()[0]
                df_clean[col].fillna(mode_val, inplace=True)
                print(f"   âœ“ {col} : {missing_count} valeurs â†’ remplies par mode ({mode_val})")
        
        elif missing_pct < 30:  # Conservation avec warning
            print(f"   âš ï¸  {col} : {missing_pct:.1f}% manquant â†’ colonne conservÃ©e avec NaN")
        
        else:  # Suppression si > 30%
            df_clean = df_clean.drop(columns=[col])
            print(f"   âŒ {col} : {missing_pct:.1f}% manquant â†’ colonne supprimÃ©e")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 6 : Suppression des valeurs aberrantes extrÃªmes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Ã‰TAPE 6 : Traitement des valeurs aberrantes")

# Suppression des valeurs nÃ©gatives si illogiques
for col in numeric_cols:
    if col in df_clean.columns:
        negative_count = (df_clean[col] < 0).sum()
        if negative_count > 0:
            df_clean = df_clean[df_clean[col] >= 0]
            print(f"   âœ“ {col} : {negative_count} valeurs nÃ©gatives supprimÃ©es")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RÃ‰SUMÃ‰ FINAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print(" " * 30 + "RÃ‰SUMÃ‰ DU NETTOYAGE")
print("="*80)
print(f"\n   ğŸ“Š Lignes avant nettoyage : {initial_rows:,}")
print(f"   ğŸ“Š Lignes aprÃ¨s nettoyage : {len(df_clean):,}")
print(f"   ğŸ“Š Lignes supprimÃ©es : {initial_rows - len(df_clean):,} ({((initial_rows - len(df_clean))/initial_rows*100):.2f}%)")
print(f"   ğŸ“Š Colonnes finales : {len(df_clean.columns)}")
print(f"   ğŸ“Š Valeurs manquantes restantes : {df_clean.isnull().sum().sum()}")
print(f"\n   âœ… Dataset nettoyÃ© et prÃªt pour l'analyse !\n")
print("="*80 + "\n")
```

**ğŸ“Š Comparaison Avant/AprÃ¨s Nettoyage**
```python
# ===============================================
# Visualisation Avant/AprÃ¨s
# ===============================================
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Graphique 1 : Valeurs manquantes avant
missing_before = df.isnull().sum()
if missing_before.sum() > 0:
    axes[0].bar(range(len(missing_before)), missing_before.values, 
                color='#e74c3c', alpha=0.8, edgecolor='black')
    axes[0].set_xticks(range(len(missing_before)))
    axes[0].set_xticklabels(missing_before.index, rotation=45, ha='right')
    axes[0].set_title('Valeurs Manquantes AVANT Nettoyage', 
                      fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Nombre de valeurs manquantes', fontsize=12)
    axes[0].grid(axis='y', alpha=0.3)
else:
    axes[0].text(0.5, 0.5, 'Aucune valeur manquante', 
                 ha='center', va='center', fontsize=14)
    axes[0].axis('off')

# Graphique 2 : Valeurs manquantes aprÃ¨s
missing_after = df_clean.isnull().sum()
if missing_after.sum() > 0:
    axes[1].bar(range(len(missing_after)), missing_after.values, 
                color='#2ecc71', alpha=0.8, edgecolor='black')
    axes[1].set_xticks(range(len(missing_after)))
    axes[1].set_xticklabels(missing_after.index, rotation=45, ha='right')
    axes[1].set_title('Valeurs Manquantes APRÃˆS Nettoyage', 
                      fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Nombre de valeurs manquantes', fontsize=12)
    axes[1].grid(axis='y', alpha=0.3)
else:
    axes[1].text(0.5, 0.5, 'âœ… Toutes les valeurs manquantes traitÃ©es', 
                 ha='center', va='center', fontsize=14, color='green', fontweight='bold')
    axes[1].axis('off')

plt.tight_layout()
plt.show()
```

---

## ğŸ“Š 5. Analyse Exploratoire des DonnÃ©es (EDA)

### 5.1 Ã‰volution Mondiale des Ventes EV
```python
# ===============================================
# Graphique 1 : Tendance Temporelle Globale
# ===============================================
print("\n" + "="*80)
print(" " * 25 + "ANALYSE 1 : Ã‰VOLUTION TEMPORELLE")
print("="*80)

# AgrÃ©gation par annÃ©e
if 'year' in df_clean.columns:
    # DÃ©tection de la colonne de ventes
    sales_cols = [col for col in df_clean.columns if 'sale' in col.lower() or 'ev' in col.lower()]
    
    if sales_cols:
        sales_col = sales_cols[0]
        yearly_sales = df_clean.groupby('year')[sales_col].sum().reset_index()
        yearly_sales = yearly_sales.sort_values('year')
        
        # Graphique principal
        fig, ax = plt.subplots(figsize=(16, 8))
        
        # Ligne principale avec remplissage
        ax.plot(yearly_sales['year'], yearly_sales[sales_col], 
                marker='o', linewidth=3, markersize=10, color='#27ae60', 
                label='Ventes Totales EV', markeredgecolor='white', markeredgewidth=2,
                zorder=3)
        
        ax.fill_between(yearly_sales['year'], yearly_sales[sales_col], 
                         alpha=0.3, color='#27ae60', zorder=1)
        
        # Mise en forme
        ax.set_title('ğŸ“ˆ Ã‰volution Mondiale des Ventes de VÃ©hicules Ã‰lectriques (2010-2024)', 
                    fontsize=18, fontweight='bold', pad=20)
        ax.set_xlabel('AnnÃ©e', fontsize=14, fontweight='bold')
        ax.set_ylabel('Nombre de VÃ©hicules Vendus', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, linestyle='--', zorder=0)
        ax.legend(fontsize=13, loc='upper left', framealpha=0.9)
        
        # Formatage des axes
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
        
        # Annotations pour points clÃ©s
        for idx in [0, len(yearly_sales)//2, len(yearly_sales)-1]:
            year_val = yearly_sales['year'].iloc[idx]
            sales_val = yearly_sales[sales_col].iloc[idx]
            
            ax.annotate(f'{sales_val:,.0f}',
                       xy=(year_val, sales_val),
                       xytext=(0, 15), textcoords='offset points',
                       ha='center', fontsize=11, fontweight='bold',
                       bbox=dict(boxstyle='round,pad=0.6', facecolor='yellow', 
                                alpha=0.8, edgecolor='black', linewidth=1.5),
                       arrowprops=dict(arrowstyle='->', lw=1.5))
        
        plt.tight_layout()
        plt.show()
        
        # Statistiques dÃ©taillÃ©es
        print(f"\nğŸ“ˆ STATISTIQUES TEMPORELLES")
        print(f"{'â”€'*80}")
        
        first_year = yearly_sales['year'].iloc[0]
        last_year = yearly_sales['year'].iloc[-1]
        first_sales = yearly_sales[sales_col].iloc[0]
        last_sales = yearly_sales[sales_col].iloc[-1]
        
        growth_rate = ((last_sales / first_sales) - 1) * 100
        cagr = ((last_sales / first_sales) ** (1/(last_year - first_year)) - 1) * 100
        
        print(f"   â€¢ AnnÃ©e de dÃ©part : {first_year}")
        print(f"   â€¢ AnnÃ©e finale : {last_year}")
        print(f"   â€¢ Ventes en {first_year} : {first_sales:,.0f} vÃ©hicules")
        print(f"   â€¢ Ventes en {last_year} : {last_sales:,.0f} vÃ©hicules")
        print(f"   â€¢ Croissance totale : {growth_rate:,.1f}%")
        print(f"   â€¢ CAGR (Taux de croissance annuel composÃ©) : {cagr:.1f}%")
        print(f"   â€¢ Multiplication : Ã—{(last_sales/first_sales):.1f}")
        
        # Analyse de la croissance annÃ©e par annÃ©e
        yearly_sales['growth'] = yearly_sales[sales_col].pct_change() * 100
        print(f"\nğŸ“Š CROISSANCE ANNUELLE")
        print(f"{'â”€'*80}")
        print(f"   â€¢ Croissance moyenne : {yearly_sales['growth'].mean():.1f}%")
        print(f"   â€¢ Croissance maximale : {yearly_sales['growth'].max():.1f}% (annÃ©e {yearly_sales.loc[yearly_sales['growth'].idxmax(), 'year']})")
        print(f"   â€¢ Croissance minimale : {yearly_sales['growth'].min():.1f}% (annÃ©e {yearly_sales.loc[yearly_sales['growth'].idxmin(), 'year']})")

print("\n" + "="*80 + "\n")
```

**ğŸ“ InterprÃ©tation** :
- **Croissance exponentielle** observÃ©e depuis 2015
- **AccÃ©lÃ©ration majeure** aprÃ¨s 2020 (politiques climatiques, subventions gouvernementales)
- **Facteurs explicatifs** : baisse des coÃ»ts des batteries, infrastructure de re5.2 Analyse par RÃ©gion/Pays
python# ===============================================
# Graphique 2 : RÃ©partition GÃ©ographique
# ===============================================
print("\n" + "="*80)
print(" " * 25 + "ANALYSE 2 : RÃ‰PARTITION GÃ‰OGRAPHIQUE")
print("="*80)

# Identification de la colonne pays/rÃ©gion
country_cols = [col for col in df_clean.columns if 'country' in col.lower() or 'region' in col.lower()]

if country_cols and sales_cols:
    country_col = country_cols[0]
    sales_col = sales_cols[0]
    
    # AgrÃ©gation par pays
    country_sales = df_clean.groupby(country_col)[sales_col].sum().sort_values(ascending=False)
    
    # Top 15 pays
    top_countries = country_sales.head(15)
    
    # Graphique
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    
    # Graphique 1 : Barres horizontales
    axes[0].barh(range(len(top_countries)), top_countries.values, 
                 color=plt.cm.viridis(np.linspace(0, 1, len(top_countries))),
                 edgecolor='black', linewidth=1.5)
    axes[0].set_yticks(range(len(top_countries)))
    axes[0].set_yticklabels(top_countries.index)
    axes[0].invert_yaxis()
    axes[0].set_xlabel('Ventes Totales EV (2010-2024)', fontsize=12, fontweight='bold')
    axes[0].set_title('ğŸŒ Top 15 Pays - Ventes Totales', fontsize=14, fontweight='bold')
    axes[0].grid(axis='x', alpha=0.3, linestyle='--')
    axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
    
    # Ajout des valeurs sur les barres
    for i, v in enumerate(top_countries.values):
        axes[0].text(v, i, f' {v:,.0f}', va='center', fontsize=10, fontweight='bold')
    
    # Graphique 2 : Camembert (Top 10)
    top10 = country_sales.head(10)
    others = country_sales[10:].sum()
    
    pie_data = pd.concat([top10, pd.Series({'Autres': others})])
    colors = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))
    
    wedges, texts, autotexts = axes[1].pie(pie_data.values, 
                                            labels=pie_data.index,
                                            autopct='%1.1f%%',
                                            startangle=90,
                                            colors=colors,
                                            textprops={'fontsize': 10, 'fontweight': 'bold'})
    
    axes[1].set_title('ğŸ“Š RÃ©partition Mondiale du MarchÃ© EV', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    # Statistiques
    print(f"\nğŸŒ STATISTIQUES GÃ‰OGRAPHIQUES")
    print(f"{'â”€'*80}")
    print(f"   â€¢ Nombre total de pays/rÃ©gions : {df_clean[country_col].nunique()}")
    print(f"   â€¢ Leader mondial : {top_countries.index[0]} ({top_countries.iloc[0]:,.0f} ventes)")
    print(f"   â€¢ Part du leader : {(top_countries.iloc[0]/country_sales.sum()*100):.1f}%")
    print(f"   â€¢ Top 3 reprÃ©sente : {(top_countries.head(3).sum()/country_sales.sum()*100):.1f}% du marchÃ©")
    
    print(f"\nğŸ“‹ TOP 10 PAYS")
    print(f"{'â”€'*80}")
    for i, (country, sales) in enumerate(top_countries.head(10).items(), 1):
        pct = (sales / country_sales.sum()) * 100
        print(f"   {i:2d}. {country:20s} : {sales:12,.0f} ventes ({pct:5.1f}%)")

print("\n" + "="*80 + "\n")
ğŸ“ InterprÃ©tation :

Chine : Leader incontestÃ© (>50% du marchÃ© mondial)
USA & Europe : MarchÃ©s matures en forte croissance
Inde & Asie du Sud-Est : MarchÃ©s Ã©mergents prometteurs

5.3 Analyse par Type de Motorisation (BEV vs PHEV)
python# ===============================================
# Graphique 3 : BEV vs PHEV
# ===============================================
print("\n" + "="*80)
print(" " * 22 + "ANALYSE 3 : TYPE DE MOTORISATION (BEV vs PHEV)")
print("="*80)

type_cols = [col for col in df_clean.columns if 'type' in col.lower() or 'powertrain' in col.lower()]

if type_cols and 'year' in df_clean.columns and sales_cols:
    type_col = type_cols[0]
    sales_col = sales_cols[0]
    
    # AgrÃ©gation par annÃ©e et type
    type_evolution = df_clean.groupby(['year', type_col])[sales_col].sum().unstack(fill_value=0)
    
    # Graphique
    fig, axes = plt.subplots(2, 1, figsize=(16, 12))
    
    # Graphique 1 : Ã‰volution en aires empilÃ©es
    type_evolution.plot(kind='area', stacked=True, ax=axes[0], 
                        alpha=0.7, linewidth=2, edgecolor='black')
    axes[0].set_title('ğŸ”‹ Ã‰volution des Ventes par Type de Motorisation', 
                      fontsize=16, fontweight='bold', pad=15)
    axes[0].set_xlabel('AnnÃ©e', fontsize=13, fontweight='bold')
    axes[0].set_ylabel('Ventes (unitÃ©s)', fontsize=13, fontweight='bold')
    axes[0].legend(title='Type de Motorisation', fontsize=11, title_fontsize=12, 
                   loc='upper left', framealpha=0.9)
    axes[0].grid(True, alpha=0.3, linestyle='--')
    axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
    
    # Graphique 2 : Ã‰volution des parts de marchÃ©
    type_pct = type_evolution.div(type_evolution.sum(axis=1), axis=0) * 100
    type_pct.plot(kind='line', ax=axes[1], marker='o', linewidth=3, markersize=8)
    axes[1].set_title('ğŸ“Š Parts de MarchÃ© par Type (%)', 
                      fontsize=16, fontweight='bold', pad=15)
    axes[1].set_xlabel('AnnÃ©e', fontsize=13, fontweight='bold')
    axes[1].set_ylabel('Part de MarchÃ© (%)', fontsize=13, fontweight='bold')
    axes[1].legend(title='Type de Motorisation', fontsize=11, title_fontsize=12, 
                   loc='best', framealpha=0.9)
    axes[1].grid(True, alpha=0.3, linestyle='--')
    axes[1].set_ylim(0, 100)
    
    plt.tight_layout()
    plt.show()
    
    # Statistiques
    print(f"\nğŸ”‹ STATISTIQUES PAR TYPE DE MOTORISATION")
    print(f"{'â”€'*80}")
    
    total_by_type = df_clean.groupby(type_col)[sales_col].sum().sort_values(ascending=False)
    
    for vehicle_type, sales in total_by_type.items():
        pct = (sales / total_by_type.sum()) * 100
        print(f"   â€¢ {vehicle_type:15s} : {sales:12,.0f} ventes ({pct:5.1f}%)")
    
    # Analyse du changement
    if len(type_evolution) > 1:
        first_year_pct = type_pct.iloc[0]
        last_year_pct = type_pct.iloc[-1]
        
        print(f"\nğŸ“ˆ Ã‰VOLUTION DES PARTS DE MARCHÃ‰")
        print(f"{'â”€'*80}")
        print(f"   AnnÃ©e {type_evolution.index[0]} â†’ AnnÃ©e {type_evolution.index[-1]}")
        
        for vtype in type_pct.columns:
            change = last_year_pct[vtype] - first_year_pct[vtype]
            print(f"   â€¢ {vtype:15s} : {first_year_pct[vtype]:5.1f}% â†’ {last_year_pct[vtype]:5.1f}% ({change:+.1f} points)")

print("\n" + "="*80 + "\n")
ğŸ“ InterprÃ©tation :

BEV (100% Ã©lectrique) : Domination croissante depuis 2019
PHEV (Hybride rechargeable) : DÃ©clin relatif mais toujours prÃ©sent
Tendance : Les consommateurs prÃ©fÃ¨rent de plus en plus les vÃ©hicules 100% Ã©lectriques

5.4 Heatmap des CorrÃ©lations
python# ===============================================
# Graphique 4 : Matrice de CorrÃ©lation
# ===============================================
print("\n" + "="*80)
print(" " * 27 + "ANALYSE 4 : CORRÃ‰LATIONS")
print("="*80)

# SÃ©lection des colonnes numÃ©riques
numeric_data = df_clean.select_dtypes(include=[np.number])

if len(numeric_data.columns) > 1:
    # Calcul de la matrice de corrÃ©lation
    corr_matrix = numeric_data.corr()
    
    # Graphique
    plt.figure(figsize=(12, 10))
    
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
    
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0,
                square=True, linewidths=2, cbar_kws={"shrink": 0.8},
                vmin=-1, vmax=1, mask=mask,
                annot_kws={'size': 10, 'weight': 'bold'})
    
    plt.title('ğŸ”— Matrice de CorrÃ©lation - Variables NumÃ©riques', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xticks(rotation=45, ha='right', fontsize=11)
    plt.yticks(rotation=0, fontsize=11)
    plt.tight_layout()
    plt.show()
    
    # Analyse des corrÃ©lations fortes
    print(f"\nğŸ”— CORRÃ‰LATIONS SIGNIFICATIVES (|r| > 0.5)")
    print(f"{'â”€'*80}")
    
    # Extraction des corrÃ©lations fortes
    corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.5:
                corr_pairs.append({
                    'Variable 1': corr_matrix.columns[i],
                    'Variable 2': corr_matrix.columns[j],
                    'CorrÃ©lation': corr_val
                })
    
    if corr_pairs:
        corr_df = pd.DataFrame(corr_pairs).sort_values('CorrÃ©lation', 
                                                        key=abs, ascending=False)
        for _, row in corr_df.iterrows():
            direction = "positive" if row['CorrÃ©lation'] > 0 else "nÃ©gative"
            print(f"   â€¢ {row['Variable 1']:20s} â†” {row['Variable 2']:20s} : {row['CorrÃ©lation']:+.3f} ({direction})")
    else:
        print("   â„¹ï¸  Aucune corrÃ©lation forte dÃ©tectÃ©e")

print("\n" + "="*80 + "\n")
ğŸ“ InterprÃ©tation :

Identification des variables redondantes (multicolinÃ©aritÃ©)
Relations entre les features pour la modÃ©lisation
SÃ©lection des variables les plus pertinentes

5.5 Analyse Temporelle AvancÃ©e
python# ===============================================
# Graphique 5 : Analyse Temporelle AvancÃ©e
# ===============================================
print("\n" + "="*80)
print(" " * 22 + "ANALYSE 5 : TENDANCES TEMPORELLES AVANCÃ‰ES")
print("="*80)

if 'year' in df_clean.columns and sales_cols:
    sales_col = sales_cols[0]
    yearly_sales = df_clean.groupby('year')[sales_col].sum().reset_index()
    
    # Calcul des mÃ©triques
    yearly_sales['growth_rate'] = yearly_sales[sales_col].pct_change() * 100
    yearly_sales['cumulative'] = yearly_sales[sales_col].cumsum()
    
    # Graphique Ã  3 sous-graphiques
    fig, axes = plt.subplots(3, 1, figsize=(16, 14))
    
    # Sous-graphique 1 : Ventes annuelles
    axes[0].bar(yearly_sales['year'], yearly_sales[sales_col], 
                color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[0].set_title('ğŸ“Š Ventes Annuelles de VÃ©hicules Ã‰lectriques', 
                      fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Ventes (unitÃ©s)', fontsize=12, fontweight='bold')
    axes[0].grid(axis='y', alpha=0.3, linestyle='--')
    axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
    
    # Sous-graphique 2 : Taux de croissance
    colors = ['#27ae60' if x > 0 else '#e74c3c' for x in yearly_sales['growth_rate'].fillna(0)]
    axes[1].bar(yearly_sales['year'], yearly_sales['growth_rate'], 
                color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)
    axes[1].set_title('ğŸ“ˆ Taux de Croissance Annuel (%)', 
                      fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Croissance (%)', fontsize=12, fontweight='bold')
    axes[1].grid(axis='y', alpha=0.3, linestyle='--')
    
    # Sous-graphique 3 : Ventes cumulÃ©es
    axes[2].plot(yearly_sales['year'], yearly_sales['cumulative'], 
                 marker='o', linewidth=3, markersize=8, color='#9b59b6',
                 markeredgecolor='white', markeredgewidth=2)
    axes[2].fill_between(yearly_sales['year'], yearly_sales['cumulative'], 
                         alpha=0.3, color='#9b59b6')
    axes[2].set_title('ğŸ“Š Ventes CumulÃ©es (Parc Total Vendu)', 
                      fontsize=14, fontweight='bold')
    axes[2].set_xlabel('AnnÃ©e', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('Ventes CumulÃ©es (unitÃ©s)', fontsize=12, fontweight='bold')
    axes[2].grid(True, alpha=0.3, linestyle='--')
    axes[2].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
    
    plt.tight_layout()
    plt.show()
    
    # Statistiques avancÃ©es
    print(f"\nğŸ“Š STATISTIQUES TEMPORELLES AVANCÃ‰ES")
    print(f"{'â”€'*80}")
    print(f"   â€¢ Ventes totales (cumul) : {yearly_sales['cumulative'].iloc[-1]:,.0f} vÃ©hicules")
    print(f"   â€¢ Moyenne annuelle : {yearly_sales[sales_col].mean():,.0f} vÃ©hicules/an")
    print(f"   â€¢ MÃ©diane : {yearly_sales[sales_col].median():,.0f} vÃ©hicules/an")
    print(f"   â€¢ Ã‰cart-type : {yearly_sales[sales_col].std():,.0f}")
    print(f"   â€¢ Coefficient de variation : {(yearly_sales[sales_col].std()/yearly_sales[sales_col].mean()*100):.1f}%")
    
    print(f"\nğŸ“ˆ ANALYSE DES TAUX DE CROISSANCE")
    print(f"{'â”€'*80}")
    growth_stats = yearly_sales['growth_rate'].dropna()
    print(f"   â€¢ Croissance moyenne : {growth_stats.mean():.1f}%")
    print(f"   â€¢ Croissance mÃ©diane : {growth_stats.median():.1f}%")
    print(f"   â€¢ Croissance maximale : {growth_stats.max():.1f}% (annÃ©e {yearly_sales.loc[growth_stats.idxmax(), 'year']})")
    print(f"   â€¢ Croissance minimale : {growth_stats.min():.1f}% (annÃ©e {yearly_sales.loc[growth_stats.idxmin(), 'year']})")
    
    # DÃ©tection des annÃ©es de rupture
    print(f"\nğŸ¯ ANNÃ‰ES CLÃ‰S")
    print(f"{'â”€'*80}")
    threshold = growth_stats.mean() + growth_stats.std()
    breakthrough_years = yearly_sales[yearly_sales['growth_rate'] > threshold]['year'].tolist()
    
    if breakthrough_years:
        print(f"   â€¢ AnnÃ©es de forte accÃ©lÃ©ration (>{threshold:.1f}%) : {', '.join(map(str, breakthrough_years))}")
    
    # Point d'inflexion
    max_growth_year = yearly_sales.loc[growth_stats.idxmax(), 'year']
    print(f"   â€¢ Point d'inflexion majeur : {max_growth_year}")

print("\n" + "="*80 + "\n")
ğŸ“ InterprÃ©tation :

Tendance exponentielle confirmÃ©e sur la pÃ©riode
VolatilitÃ© de la croissance selon les annÃ©es
Points d'inflexion identifiÃ©s (Ã©vÃ©nements majeurs, politiques publiques)


ğŸ¤– 6. PrÃ©paration pour la ModÃ©lisation
6.1 Encodage des Variables CatÃ©gorielles
python# ===============================================
# PrÃ©paration des DonnÃ©es pour ML
# ===============================================
print("\n" + "="*80)
print(" " * 22 + "PRÃ‰PARATION POUR LA MODÃ‰LISATION")
print("="*80)

# Copie pour modÃ©lisation
df_model = df_clean.copy()

print("\nğŸ”§ Ã‰TAPE 1 : Encodage des variables catÃ©gorielles")
print("â”€"*80)

# Identification des colonnes catÃ©gorielles
categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()

if categorical_cols:
    print(f"   â€¢ Colonnes catÃ©gorielles identifiÃ©es : {', '.join(categorical_cols)}")
    
    # One-Hot Encoding
    df_encoded = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)
    
    print(f"   âœ“ Encodage One-Hot appliquÃ©")
    print(f"   âœ“ Colonnes avant : {len(df_model.columns)}")
    print(f"   âœ“ Colonnes aprÃ¨s : {len(df_encoded.columns)}")
    print(f"   âœ“ Nouvelles features crÃ©Ã©es : {len(df_encoded.columns) - len(df_model.columns)}")
else:
    df_encoded = df_model.copy()
    print("   â„¹ï¸  Aucune variable catÃ©gorielle Ã  encoder")

print(f"\nğŸ“‹ AperÃ§u du dataset encodÃ© :")
display(df_encoded.head())
6.2 Normalisation des Variables
pythonprint("\nğŸ”§ Ã‰TAPE 2 : Normalisation des variables numÃ©riques")
print("â”€"*80)

from sklearn.preprocessing import StandardScaler

# Identification des colonnes numÃ©riques
numeric_cols_model = df_encoded.select_dtypes(include=[np.number]).columns.tolist()

if numeric_cols_model:
    # Initialisation du scaler
    scaler = StandardScaler()
    
    # Application de la normalisation
    df_normalized = df_encoded.copy()
    df_normalized[numeric_cols_model] = scaler.fit_transform(df_encoded[numeric_cols_model])
    
    print(f"   âœ“ Normalisation StandardScaler appliquÃ©e")
    print(f"   âœ“ Colonnes normalisÃ©es : {len(numeric_cols_model)}")
    
    # Statistiques avant/aprÃ¨s
    print(f"\nğŸ“Š Statistiques AVANT normalisation :")
    display(df_encoded[numeric_cols_model].describe().loc[['mean', 'std']])
    
    print(f"\nğŸ“Š Statistiques APRÃˆS normalisation :")
    display(df_normalized[numeric_cols_model].describe().loc[['mean', 'std']])
else:
    df_normalized = df_encoded.copy()
    print("   â„¹ï¸  Aucune variable numÃ©rique Ã  normaliser")
6.3 SÃ©paration Train/Test
pythonprint("\nğŸ”§ Ã‰TAPE 3 : SÃ©paration Train/Test")
print("â”€"*80)

from sklearn.model_selection import train_test_split

# Identification de la variable cible
target_cols = [col for col in df_normalized.columns if 'sale' in col.lower() or 'target' in col.lower()]

if target_cols:
    target_col = target_cols[0]
    
    # SÃ©paration X et y
    X = df_normalized.drop(columns=[target_col])
    y = df_normalized[target_col]
    
    # Split 80/20
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    print(f"   âœ“ Variable cible : {target_col}")
    print(f"   âœ“ Features (X) : {X.shape[1]} colonnes")
    print(f"   âœ“ Target (y) : {target_col}")
    print(f"\n   ğŸ“Š RÃ©partition Train/Test :")
    print(f"      â€¢ Train set : {X_train.shape[0]} lignes ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"      â€¢ Test set  : {X_test.shape[0]} lignes ({X_test.shape[0]/len(X)*100:.1f}%)")
    
    print(f"\n   âœ… Dataset prÃªt pour l'entraÃ®nement de modÃ¨les ML !")
else:
    print("   âš ï¸  Impossible d'identifier la variable cible automatiquement")

print("\n" + "="*80 + "\n")
6.4 Exemple de ModÃ©lisation (Bonus)
python# ===============================================
# Exemple : ModÃ¨le de RÃ©gression (Optionnel)
# ===============================================
print("\n" + "="*80)
print(" " * 25 + "EXEMPLE DE MODÃ‰LISATION")
print("="*80)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("\nğŸ¤– EntraÃ®nement d'un modÃ¨le Random Forest Regressor")
print("â”€"*80)

# Initialisation du modÃ¨le
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)

# EntraÃ®nement
rf_model.fit(X_train, y_train)
print("   âœ“ ModÃ¨le entraÃ®nÃ© avec succÃ¨s")

# PrÃ©dictions
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Ã‰valuation
print(f"\nğŸ“Š PERFORMANCE DU MODÃˆLE")
print(f"{'â”€'*80}")

# MÃ©triques Train
mae_train = mean_absolute_error(y_train, y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
r2_train = r2_score(y_train, y_pred_train)

print(f"\n   ğŸ¯ Train Set :")
print(f"      â€¢ MAE  : {mae_train:,.2f}")
print(f"      â€¢ RMSE : {rmse_train:,.2f}")
print(f"      â€¢ RÂ²   : {r2_train:.4f}")

# MÃ©triques Test
mae_test = mean_absolute_error(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
r2_test = r2_score(y_test, y_pred_test)

print(f"\n   ğŸ¯ Test Set :")
print(f"      â€¢ MAE  : {mae_test:,.2f}")
print(f"      â€¢ RMSE : {rmse_test:,.2f}")
print(f"      â€¢ RÂ²   : {r2_test:.4f}")

# Feature Importance
print(f"\nğŸ” TOP 10 FEATURES LES PLUS IMPORTANTES")
print(f"{'â”€'*80}")

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

for i, row in feature_importance.head(10).iterrows():
    print(f"   {row['Feature']:30s} : {row['Importance']:.4f}")

# Graphique Feature Importance
plt.figure(figsize=(12, 8))
top_features = feature_importance.head(15)
plt.barh(range(len(top_features)), top_features['Importance'].values,
         color='steelblue', alpha=0.8, edgecolor='black', linewidth=1.5)
plt.yticks(range(len(top_features)), top_features['Feature'].values)
plt.xlabel('Importance', fontsize=12, fontweight='bold')
plt.title('ğŸ” Top 15 Features - Importance dans le ModÃ¨le Random Forest', 
          fontsize=14, fontweight='bold', pad=15)
plt.grid(axis='x', alpha=0.3, linestyle='--')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\n" + "="*80 + "\n")
ğŸ“ InterprÃ©tation :

RÂ² proche de 1 : Excellent ajustement du modÃ¨le
Feature Importance : Identification des variables clÃ©s
DiffÃ©rence Train/Test : Ã‰valuation du surapprentissage


ğŸ“ 7. Conclusions et Recommandations
7.1 SynthÃ¨se des Insights ClÃ©s
pythonprint("\n" + "="*80)
print(" " * 28 + "RAPPORT FINAL - INSIGHTS CLÃ‰S")
print("="*80)

print("\n1ï¸âƒ£  VUE D'ENSEMBLE DU MARCHÃ‰ EV")
print("â”€"*80)
print("   âœ“ Croissance exponentielle du marchÃ© des vÃ©hicules Ã©lectriques")
print("   âœ“ PÃ©riode analysÃ©e : 2010-2024 (15 ans)")
print("   âœ“ Multiplication des ventes par un facteur significatif")
print("   âœ“ AccÃ©lÃ©ration majeure observÃ©e aprÃ¨s 2020")

print("\n2ï¸âƒ£  LEADERS GÃ‰OGRAPHIQUES")
print("â”€"*80)
print("   âœ“ Chine : Leader mondial incontestÃ© (>50% du marchÃ©)")
print("   âœ“ Europe : MarchÃ© mature avec forte croissance")
print("   âœ“ USA : Adoption rapide mais retard relatif")
print("   âœ“ MarchÃ©s Ã©mergents : Inde, Asie du Sud-Est en progression")

print("\n3ï¸âƒ£  Ã‰VOLUTION TECHNOLOGIQUE")
print("â”€"*80)
print("   âœ“ BEV (100% Ã©lectrique) : Domination croissante")
print("   âœ“ PHEV (Hybride rechargeable) : DÃ©clin relatif")
print("   âœ“ PrÃ©fÃ©rence consommateur : Vers le tout-Ã©lectrique")
print("   âœ“ Innovation : AmÃ©lioration de l'autonomie et baisse des coÃ»ts")

print("\n4ï¸âƒ£  FACTEURS DE SUCCÃˆS")
print("â”€"*80)
print("   âœ“ Politiques publiques : Subventions, bonus Ã©cologiques")
print("   âœ“ Infrastructure : DÃ©ploiement des bornes de recharge")
print("   âœ“ Technologie : Baisse du coÃ»t des batteries lithium-ion")
print("   âœ“ Conscience environnementale : Demande croissante")

print("\n5ï¸âƒ£  QUALITÃ‰ DES DONNÃ‰ES")
print("â”€"*80)
print(f"   âœ“ Dataset nettoyÃ© et structurÃ©")
print(f"   âœ“ Valeurs manquantes traitÃ©es")
print(f"   âœ“ Variables encodÃ©es et normalisÃ©es")
print(f"   âœ“ PrÃªt pour la modÃ©lisation prÃ©dictive")

print("\n" + "="*80)
7.2 Recommandations StratÃ©giques
pythonprint("\n" + "="*80)
print(" " * 25 + "RECOMMANDATIONS STRATÃ‰GIQUES")
print("="*80)

print("\nğŸ¯ POUR LES CONSTRUCTEURS AUTOMOBILES")
print("â”€"*80)
print("   1. AccÃ©lÃ©rer la transition vers les BEV (100% Ã©lectrique)")
print("   2. Investir massivement en R&D battCette rÃ©ponse a Ã©tÃ© interrompue car Claude a atteint sa longueur maximale de message. Cliquez sur continuer pour permettre Ã  Claude de poursuivre.ContinuerClaude peut faire des erreurs. Assurez-vous de vÃ©rifier ses rÃ©ponses.
ğŸ§© Nouvelle Section : Comment utiliser ce code ? (Guide dâ€™ExÃ©cution)
âš™ï¸ Objectif de cette section

Cette section explique comment exÃ©cuter le code fourni, dans quel environnement, et ce que fait chaque partie du script.
Elle sert de guide pratique, surtout si tu veux partager ce rapport avec dâ€™autres Ã©tudiants ou collÃ¨gues.

ğŸ› ï¸ Utilisation du Code : Environnement, PrÃ©requis et Instructions
1ï¸âƒ£ Environnement RecommandÃ©

Ce code est optimisÃ© pour Ãªtre exÃ©cutÃ© dans :

Google Colab (fortement recommandÃ©)

Jupyter Notebook (Anaconda)

VS Code + Python

ğŸ’¡ Google Colab est idÃ©al car KaggleHub y fonctionne sans configuration complexe.

2ï¸âƒ£ PrÃ©requis : Installer les DÃ©pendances

Le code commence par installer toutes les bibliothÃ¨ques nÃ©cessaires :

kagglehub : pour importer le dataset depuis Kaggle automatiquement

pandas / numpy : manipulation des donnÃ©es

matplotlib / seaborn : visualisation

scikit-learn : prÃ©paration pour la modÃ©lisation

!pip install kagglehub[pandas-datasets]
!pip install pandas numpy matplotlib seaborn scikit-learn


ğŸ‘‰ Tu dois exÃ©cuter ce bloc une seule fois au dÃ©but du notebook.

3ï¸âƒ£ Structure Globale du Code

Le script complet contient 7 grandes parties, chacune correspondant Ã  une Ã©tape dâ€™un pipeline de data science :

Section	Description
1. Installation	Installe tous les packages
2. Importation	Charge les bibliothÃ¨ques
3. Chargement	TÃ©lÃ©charge et lit le dataset
4. Exploration	VÃ©rifie la structure et la qualitÃ©
5. Nettoyage	CrÃ©e une version propre du dataset
6. EDA	Graphes et analyses
7. ModÃ©lisation	PrÃ©paration pour ML
4ï¸âƒ£ Comment exÃ©cuter chaque bloc ?
ğŸ”¹ Ã‰tape 1 : Importer les bibliothÃ¨ques
import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


ğŸ“Œ Ce bloc prÃ©pare l'espace de travail.

ğŸ”¹ Ã‰tape 2 : Charger le dataset depuis Kaggle
df = kagglehub.load_dataset(
    KaggleDatasetAdapter.PANDAS,
    "patricklford/global-ev-sales-2010-2024",
    ""
)


ğŸ“Œ Ce code :

tÃ©lÃ©charge automatiquement les fichiers depuis Kaggle

lit directement le dataset en DataFrame

affiche la taille et les premiÃ¨res lignes

ğŸ‘‰ Pas besoin de clÃ© API Kaggle, contrairement Ã  lâ€™ancienne mÃ©thode.

ğŸ”¹ Ã‰tape 3 : Effectuer lâ€™exploration initiale

Le code :

affiche les colonnes

dÃ©tecte les valeurs manquantes

analyse les types de variables

montre les statistiques descriptives

ğŸ“Œ Cela permet de comprendre la structure avant d'appliquer un nettoyage.

ğŸ”¹ Ã‰tape 4 : Nettoyer les donnÃ©es

Dans cette partie :

suppression des doublons

normalisation des noms de colonnes

conversion des types

traitement des valeurs manquantes

suppression des valeurs aberrantes

crÃ©ation dâ€™un dataset propre : df_clean

ğŸ“Œ Cette Ã©tape transforme des donnÃ©es brutes en donnÃ©es fiables.

ğŸ”¹ Ã‰tape 5 : Visualisations et Analyse Exploratoire (EDA)

Le code gÃ©nÃ¨re :

tendances des ventes (ligne)

rÃ©partition par rÃ©gion

comparaison BEV vs PHEV

relations entre variables

ğŸ“Œ Les graphes permettent d'interprÃ©ter facilement le marchÃ© EV.

ğŸ”¹ Ã‰tape 6 : PrÃ©paration Ã  la ModÃ©lisation

Ã€ la fin, les donnÃ©es propres (df_clean) sont prÃªtes pour :

rÃ©gression

sÃ©ries temporelles

clustering

arbres de dÃ©cision

modÃ¨les avancÃ©s (XGBoost, Prophet, LSTM)

ğŸ“Œ Tu nâ€™as quâ€™Ã  ajouter ton modÃ¨le aprÃ¨s cette Ã©tape.

5ï¸âƒ£ RÃ©sultat Final

AprÃ¨s lâ€™exÃ©cution complÃ¨te :

df_clean = dataset nettoyÃ©

Graphiques = tendances et insights

Code = prÃªt pour construire un modÃ¨le ML

ğŸ“ RÃ©sumÃ© de la Section

âœ” Ce guide tâ€™aide Ã  comprendre comment fonctionne chaque partie du code
âœ” Tu peux maintenant exÃ©cuter toutes les cellules dans lâ€™ordre
âœ” Tu sais comment modifier ou Ã©tendre ce projet pour faire de la modÃ©lisation machine learning
